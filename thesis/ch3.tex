%   MSc Business Analytics Dissertation
%
%   Title:     Aaa Bbbbbbb Cccccccccc
%   Author(s): Xxxxxx Xxxxxxxxx and Yyy Yyyyyyyyy
%
%   Chapter 4: Methodology
%
%   Change Control:
%   When     Who   Ver  What
%   -------  ----  ---  --------------------------------------------------------------
%   11Feb11  AB    0.1  Begun 
%

\chapter{Methodology}\label{C.Methodology}
\section{Introduction}
{This chapters outlines the methods followed during this analysis, with a view to explaining the steps taken in detail and to aid replication. The Knowledge Discovery in Databases (KDD) process as outlined by \cite{fayyad1996kdd} was followed where possible. This is a well established end-to-end framework for deriving knowledge from raw data. To that end, data acquisition and the raw data characteristics are discussed first. This is followed by a discussion of the preprocessing and reduction required to make this data useable, including how missing data and outliers are handled. An outline of the data mining techniques employed is given, including analysis of the advantages and disadvantages of the various statistical methods and associated software packages available. Following this, the steps taken for each element of this study are outlined in detail. That is, the replication of and expansion on the work by \cite{moldovan2015learning} and the application of causal research in this domain. An analysis of the methods for interpreting results is given, including what measures of algorithmic success should be used and related matters. Overall, this chapter represents the technical aspect of this study and aims to facilitate the replication and expansion of this analysis by future researchers.} \\\\
{Git and GitHub was used as a version control and task tracking tool. All code modules written in fulfilment of this study, including all source files of this report, are available at \url {https://github.com/ReidConor/dissertation}. Using source control has various uses, not least acting as a cloud storage mechanism in case of local machine failure. In addition, changes to the project over time can be much more easily managed, including changes to the datasets used. Making this code available on GitHub also facilitates collaboration and the communication of ideas between collaborators.   }
\section{Data Acquisition}
\subsection{Core Data}
{The primary source of data for the current analysis comes from the authors of the paper it extends. Darie Moldovan and Simona Mutu were kind enough to provide the data they used in their analysis, providing the complete dataset and granting permission to use it here. This is highly beneficial for a number of reasons. Firstly, any results derived here can be placed in a much clearer context, since we can directly and numerically compare the findings of this study to the original and identify areas of achieved improvement. Secondly, being granted access to a purpose built dataset prior to undertaking this analysis represents a significant catalyst for progress and expedites the process of gaining greater understanding in this domain. The statements made by the authors based on identified correlations can also be used in the causal inference stage of this study as the basis for the formulation of research questions.}\\\\
{Three datasets where provided by Darie Moldovan and Simona Mutu, each covering 52 features for three distinct stock indexes. They are; the S\&P 500 based in the United States, the STOXX 600 based in Europe and the STOXX 300 based in Eastern Europe. Combined, these datasets total 1400 records of companies from the year 2014. The authors scrapped this data using the Bloomberg financial data repository, which contains a vast amount of historical financial data on companies across the world. In their study, the authors decided to analyse each market in isolation rather than in combination, inferring that the relationship between corporate governance and performance is characteristically different between markets.   }
\subsection{New Factors - Independent}\label{NewFactors_Independent}
{Part of this study is the exploration of new factors that could be introduced into the analysis to better explain corporate success. In other words, new independent variables to append onto the core dataset that extend the original research. A Bloomberg terminal was used to acquire all data outlined in this section due to the ease at which features could be found, extracted and integrated with the original dataset.}\\\\
{Discussed in section \ref{EnvironmentalConsiderations} is the importance of environmental performance and its influence on overall economic health. To this end an exploration of the data available in Bloomberg was conducted, however it was found that the amount of missing data for the majority of environment-related features in the year in question was prohibitive to their inclusion. Thus, it was decided to use a propriety score formulated by Bloomberg themselves as a proxy for performance in this area. The justification for this is outlined in section \ref{ESGDisclosure}.}\\\\
{Another new independent feature introduced here is total CEO compensation for each company in this study, as discussed in section \ref{execComp}. CEO compensation is readily available in Bloomberg for the year under study, and so it is relatively easy to extract and append this measure onto the core dataset.}
\subsection{New Factors - Dependent}\label{NewFactors_Dependent}
{Another goal of this study is to explore other dependant variables, or in other words auxiliary indicators that characterise company outcomes. A single new dependent variable is included, namely the Beneish M Score as outlined in section \ref{FinancialRatios}. The M Score uses an aggregate of various company-specific financial ratios to calculate the probability of that company having intentionally manipulated it's reported earnings. There are two variations on this score, one using a combination of five financial ratios and the other adding an additional three. All variables were derived from Bloomberg for the appropriate years, and appended to the original dataset.   
}
\section{Data Pre-Processing and Reduction}
\subsection{Missing Values}\label{MissingValues}
{\cite{moldovan2015learning} decided to remove any observations in their data that had missing values in the dependant variable, or not enough information to calculate those values. It could be argued that these emissions are justified, since incomplete data could unfairly skew the properties of that observation and misrepresent it in the data. Any conclusions that were made using these observations could be fundamentally flawed. Below is table outlining the degree of missing dependant variables per dataset. 
\\\\
\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}|  }
 \hline
 Dataset & Row Count & Missing Tobins Q Score & Missing Altman Z Score\\
 \hline
 SPX & 500 & 4  & 81 \\
 SXXP &   600  & 4  & 127 \\
 EEBP & 300 & 3 &  65 \\
 \hline
\end{tabular}\\\\
For the classification stage of the current study, where an attempt is made to replicate the findings of the original authors, these rows will be removed in the same way. \\\\
However when it comes to the regression and causal estimation stage, missing data represents a more complex issue. \cite{hortonMissing} state that it is critically important to address missing data, particularly in observational analysis with may predictors (as in the current study), as it arises frequently in almost all investigations using real world data. There are a number of reasons for the presence of missing data, from randomly missing data points (i.e. the propensity to be missing is independent of the value itself) to non-randomly missing data points (where the true value influences the propensity for it be to missing). The table below outlines the degree of missing values in each dataset by listing the dimensions of each if complete-case analysis were carried out. \\
\begin{table}[h!]
\centering
\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|  }
 \hline
 Dataset & Row Count & Complete Cases\\
 \hline
 SPX & 500 & 56  \\
 SXXP &   600  &  2 \\
 EEBP & 300 & 0  \\
 \hline
\end{tabular}
\end{table}\\
It is clear that complete case analysis is infeasible here, and so some other method of handling incomplete data is required. \cite{hortonMissing} discuss a range of methods for addressing this issue, with the specific aim of enabling the fitting of a logistic regression model on a sample dataset. One such method is multiple imputation, which they describe as a multi-step approach to estimating incomplete data that relies on an assumption that values are missing at random. First, the missing entries are filled in $m$ times. These new values are drawn from a distribution that is different for each entry and variable. There is then an analysis stage where the $m$ completed data sets are studied in isolation. Finally, the $m$ datasets are pooled into a final result. \cite{rubin2004multiple} states that if the imputation method is correctly implemented, then the resulting dataset is valid for statistical modelling.   \\\\
The companies represented in these datasets are public, and so are responsible for accurately reporting along a number of dimensions like company directorship and board composition, as well as financial statements which are audited by a third party. However this does not cover all features involved in this study, which may be optional for reporting purposes. Regulation in this regard also differs between markets involved in this analysis, making it difficult to deduce whether data is missing at random or not. \cite{jakobsen2017and} state that in this case, multiple imputation may be suitable. \\
A popular implementation of this technique is the \texttt{MICE} package in \texttt{R}, as outlined by \cite{mice}. \texttt{MICE} imputes using chained equations, which involves specifying the imputation model on a variable by variable basis and using the other variables as predictors. At each step in the algorithm, an imputed value is generated and used in the imputation of the next variable. This process is repeated for each iteration, until convergence is reached as specified by the Gibbs sampling procedure, outlined in more detail by \cite{yildirimGibbs}. This method can handle both continuous and discrete variables, as is required with the present data. \texttt{MICE} will be used to prepare each dataset for the regression as well as causal inference stages of the current study. 
\subsection{Outliers}\label{outliers}
\cite{moldovan2015learning} state that they remove outliers in the data, citing a desire to remove {\it ``data errors''}. They do not provide an explanation of what characterises an outlier or a data error, nor do they present any evidence that outliers are fair omissions. It is generally accepted that outliers must be proven to be mistakes at the data collection stage, or invalid in some other way to justify leaving them out of the analysis. Without such justification, outliers are valid data points and may prove crucial to the formulation of a faithful model. While they state that removal only discounts 122 observations in total, this amounts to roughly 9\% of the original dataset.\\\\
Since outlier detection and omission can have a significant impact on model performance, as shown for example by \cite{Pollet2017} and \cite{Wobbe2011}, this study will conduct identical analysis with and without the presence of outliers in order to assess the utility of their inclusion. As mentioned above, the original study neglects to detail how the authors characterised or identified outliers, and so some methodology for doing so must be chosen. \cite{denisCookReview} reviews several different methods of outlier detection, one of which is Cook's distance originally proposed by \cite{cookOrig}. Cook's distance considers the influence of a given case $i$ on all $n$ fitted values in a regression analysis, and is calculated as
\begin {equation}\label{CooksDistance}
D_i = \frac{e^2_i}{pMSE} \Big(  \frac{h_ii}{(1 - h_ii)^2}  \Big)
\end{equation}\\
where $e_i$ is the $i^{th}$  element of the residual vector, $h_ii$ is known as the leverage and $MSE$ is the mean square error. As noted before, both original dependent variables (the Tobins Q score and Altman Z score) are continuous in nature. \cite{moldovan2015learning} threshold on this value to transform the problem to a classification task, whereas the current study both replicates this and performs regression on the original values. For this reason, a method to identify outliers that relies on regression analysis is chosen, and is used before any thresholding takes place. 
\subsection{Thresholding}
{In their analysis, \cite{moldovan2015learning} threshold on both the Tobins Q score and Altman Z score to create classes from the continuous measures. This frames the problem as a classification task rather than regression which might be a more natural framing. Similar classes are created here, to facilitate a comparison of results. The original continuous measures are retained to facilitate regression analysis.  \\\\
%The unaltered, continuous versions of both ratios is also retained for regression analysis. For the causal estimation stage, thresholding is used to construct a binary dependent variable. \\\\
Tobins Q is discretised using the median to split observations into two classes, as per the original authors who cite \cite{creamer2010learning} as suggesting such a split. This is suitable for both the classification stage and causal estimation stage. The discretisation of the Altman Z score however is more involved. \cite{moldovan2015learning} create three classes here, as suggested by \cite{altman1968financial}. Those classes are listed as ``distress'', ``grey'' and ``safe'' referring to the company's risk of bankruptcy. The following values are used to create these classes. \\
\begin{table}[h!]
\centering
\begin{tabular}{ |p{4cm}|p{3cm}|  }
 \hline
 Threshold & Class \\
 \hline
 AZS \textgreater 2.99 & Safe  \\
 2.99 \textgreater AZS \textgreater 1.81 &   Gray  \\
 1.81 \textgreater AZS  & Distress   \\
 \hline
\end{tabular}
\caption{Altman Z score classes}
\end{table}\\
This is suitable for the classification stage of the current study. However, the causal estimation stage requires a binary class as the dependant variable and so the ``grey'' and ``distress'' classes are merged into one. Causal results referring to the Altman Z score thus refer to estimating the effect of some treatment on a ``safe'' or ``not safe'' level of bankruptcy risk.} 
\section{Data Mining, Algorithms and Software}
{As referenced numerous times above, there are three main stages to this study; classification, regression and causal estimation. Each of these steps requires a distinct approach and choice of toolset and software. Here, a discussion on these choices is included with justification for each.}
\subsection{Classification}\label{methodClass}
{\cite{moldovan2015learning} in the original study approached the research question as a classification problem, thresholding on the continuous dependant variables and using appropriate algorithms to achieve that goal. For each dataset and measure of success, they implemented four distinct classification algorithms and compared performance between each using identical metrics. This study implements a subset of these algorithms to facilitate a limited verification and comparison. Since the main objectives of this study lie elsewhere, this study neglects to include each and every algorithm used originally.   \\\\
The \texttt{Adaboost M1} algorithm proved to be one of the highest performing implementations in the original study, and so is included here also. The \texttt{adabag} package in \texttt{R} is used, which implements the algorithm as proposed by \cite{freundAdaboost}. As per the name, this algorithm uses {\it boosting}, which the authors state can be used to significantly reduce the error of weak learners by unifying them in a weighted sum that represents the final output of the boosted classifier. In this sense, the performance of each individual weak learner can be poor, however as long as each is better than a random guess the final result converges to what is known as a {\it strong learner}. \texttt{Adaboost} represents an improvement on bagging, attempting to build multiple models using randomly chosen training instances and eventually combining into a single model with improved accuracy. Adaboost adds an adaptive layer to this, by disproportionally weighting up poorly modelled instances (those with higher error) in subsequent models.} \\\\
{The next most performant algorithm used by \cite{moldovan2015learning} was \texttt{J48}, which is a \texttt{Java} implementation of the \texttt{C4.5} decision tree algorithm. This algorithm builds decision trees from training data using information entropy, which represents the expected value of a random variable that describes the amount of information contained in a particular split or location in a decision tree. In this way the algorithm choses attributes of the data that most effectively and efficiently splits the samples into subsets enriched in one class. Since the current study uses \texttt{R} rather than \texttt{Java}, an translated implementation is required. The \texttt{C5.0} algorithm was chosen, which is a next generation version of the \texttt{C4.5} algorithm, implementing various runtime efficiencies in terms of speed and memory usage.}
\subsection{Regression}
{For the regression stage of this analysis, both the Tobins Q score and the Altman Z score remain as continuous variables which replace the thresholded values used above.  A standard linear model is of the form
\begin {equation}\label{linearModel}
Y_i = X_i^T \beta + \epsilon_i, \qquad \epsilon_i \sim N(0, \sigma^2), \ i = 1,....n
\end{equation}\\
The optimal linear unbiased estimator for $\beta$ is found by solving 
\begin {equation}\label{optimalBeta}
(X^T X)\beta = X^T y
\end{equation}\\
For equation \ref{optimalBeta} to be solvable, the matrix $X^T X$ must be invertible which is not the case when the number of independent variables is much larger than the number of observations in the dataset or if there is collinearity between variables that are previously believed to be independent. In the current dataset, the first condition is not met although the ratio of observations to features is as low as approximately 6:1 for the STOXX 300 dataset and so may still be a valid concern. Collinearity within this dataset is certainly a source of concern, due to the large number of features included. Analysis showed that of 1830 possible pairings of features in the in all three datasets, there were 796, 944 and 392 pairings with a statistically significant collinearity coefficient respectively. A high degree of collinearity between independent variables can cause the regression coefficients to become very sensitive to small changes in the model. It can also reduce the precision of the estimate coefficients, reducing the statistical power of the model.  It is clear then that a regression methodology capable of dealing with this is required, that uses an alternative estimator for $\beta$. \\\\
\cite{hoerlRidge} proposed a modification of equation \ref{optimalBeta} based on a perturbation, denoted by $\lambda$, to the matrix $(X^T X)$ making it invertible. The equation thus becomes
\begin {equation}\label{optimalBetaPurt}
(X^T X + \lambda I )\beta = X^T y
\end{equation}\\
where $I$ is the identity matrix. The estimator of $\beta$ thus becomes
\begin {equation}\label{ridgeEstimator}
\hat{\beta} (\lambda) =  (X^T X + \lambda I)^{- 1}X^T y
\end{equation}\\
and is called the {\it ridge estimator}. \cite{hoerlRidge} state that there exists some constant $\lambda$ that leads to a mean square error less than that achieved by ordinary least squares regression. This holds even when the original matrix in equation \ref{optimalBeta} is in fact invertible. The question now arises about how to chose the optimal value for $\lambda$. There is much research on this topic, however for the current study the software will be computing values for $\lambda$ itself taking into account the fact that we require the $MSE$ to be as small as possible.   \\\\
A shortcoming of ridge regression is that is does not inherently include any variable selection stage, in that it estimates coefficients but does not act to reduce any to exactly zero and thus disregard them entirely. This can be considered suboptimal when only a few predictors are likely to be influential or where model interpretability is important. Lasso regression (least absolute shrinkage and selection operator) aims to address these issues, by performing both the regularisation step carried out by ridge regression as well as a variable selection stage. \cite{fonti2017feature} state that feature selection is a vital step in data analysis, and can act to simplify the final model by removing features that are not important as well as reducing the size of the problem to enable other algorithms to operate more quickly. Lasso works to minimise the sum of the squared errors with an upper bound on the total sum of the (absolute) values of the actual model parameters.   \\\\
The current study implements both lasso and ridge regression under the general umbrella of regularised linear regression on all three dependent variables (Tobins Q and Altman Z scores, as well as the new Benish M-Score where available). The \texttt{glmnet} package in \texttt{R} is used to achieve this, which fits a generalised linear model using a regularisation parameter \texttt{alpha}. \texttt{Alpha} dictates where along the spectrum between lasso and ridge the penalty factor lies. A value of 1 leads the algorithm to use lasso, a value of 0 leads the algorithm to use ridge. Values inbetween lead to elastic net regression, described by \cite{fonti2017feature} as a combination of the two. For each dataset and dependent variable, a series of models are built using a range of \texttt{alpha} values between 0 and 1 in 0.1 increments. Analysis is then carried out to see which implementation minimises the $MSE$. }
\subsection{Causal Estimation} \label{methodCausal}
{For the causal estimation portion of this study, a propensity score matching algorithm is used as discussed in section \ref{InferringCausation}. In order to achieve this, a module named {\texttt{causality}} written in {\texttt{python}} and made available in GitHub is used. This project is available at \url{https://github.com/akelleh/causality}. Among other pieces of functionality, this module allows the direct formulation and execution of propensity score matching and provides an interface for graphically judging the quality of the matching process. {\texttt{causality}} was installed using {\texttt{pip}}, and interfaced with using custom written {\texttt{R}} and {\texttt{python}} scripts that prepared the data, executed the module and collected the results.   \\\\
The first step in this stage of the study is the formulation of research questions, many of which are derived from the correlations and {\it 'if-this-then-that'} style conclusions of \cite{moldovan2015learning}. Many of those conclusions are market specific. However in this study each statement is tested across all markets in order to verify the applicability of each on a global scale. To those statements, this study adds new research questions motivated by existing research in this domain outlined in the literature review. A list and discussion of those questions can be found in section \ref{CausalEstimation-ResearchQuestions}.\\\\
Each research question informs the choice of {\it treatment} and {\it outcome} pairs. Treatments are variables contained in the data already and represent some condition that may cause a variation in the outcome. The outcome then is another term for a dependant variable, as used in both the classification and regression stages of this study. We use both the Tobin Q score and Altman Z score, as well as the Benish M-Score where possible.\\\\
For each pair, a collection of variables to control for are selected. Note that propensity score matching requires the control on variables that characterise whether a unit receives the treatment or not, so that the effect of the treatment on the outcome can be isolated. Thus, a classifier was built in a very similar manner to the method outlined in section \ref{methodClass} to model the allocation of the treatment in each case. The variables that are reported as important by that model are then selected as the variables to control for in the matching algorithm. The algorithm implemented in this study requires a binary outcome variable. Thus, when Tobin's Q score is used as the outcome, the binary class as per the original paper by \cite{moldovan2015learning} is used. When the Altman Z score is used as the outcome, the ``distress'' and ``grey'' risk bands are merged into one yielding a binary class that can be interpreted as ``safe'' and ``not safe'' risk levels.   }
\section{Causal Estimation - Motivating Statements}\label{CausalEstimation-ResearchQuestions}
{\cite{moldovan2015learning} make eight statements regarding the influences of corporate governance on corporate success. To these, the current study adds two that are based on findings contained in the literature that warrant further investigation. All are listed below, each referring to the market that the finding was original attributed to. \cite{moldovan2015learning} can be assumed to be the authors unless otherwise stated.   }
\begin{enumerate}
  \item For the American companies inside the S\&P 500 index, we found a positive correlation between the percentage higher than 20pct of women in the board and the Tobins Q ratio. \label{spOne}
  \item For the American companies inside the S\&P 500 index, we found...the presence of an independent lead director in the company along with a financial leverage higher than 2.5 incur a higher risk of bankruptcy. \label{spTwo}
  \item When analysing the Eastern European companies data, we found that a smaller age range for the board members is positively related with the companies performance. \label{eastOne}
  \item When analysing the Eastern European companies data, we found...that a financial leverage less than 4 is needed in order to be on the upper side of the Tobins Q ratio. \label{eastTwo}
  \item When analysing the Eastern European companies data, we found...to be on the safe zone of the Altman Z-score it is important to have an independent chairperson or even a woman as CEO.  \label{eastThree}
  \item For the Western European companies....the presence of an independent lead director or a former CEO in the board could be a sign of weaker performances, being negatively correlated with Tobins Q  \label{westOne}
  \item For the Western European companies.... a large percentage of women in the board could also affect negatively the performance. \label{westTwo}
  \item For the Western European companies....for the companies with large financial leverage in order to be in the ``safe'' zone of the Altman Z-score it could be a good idea to adopt an Auditing Committee with more than four people. \label{westThree}
  \item  \cite{coreCompensation} state that firms with weaker corporate governance underperform on the stock market, and are more likely to rewarded CEO with greater levels of compensation. \label{wildOne}
  \item \cite{fatemiESG} state that strong ESG (Environmental, Social and Governance) performance is associated with higher firm valuations.\label{wildTwo}
\end{enumerate}
\section{Interpretation}
\subsection{Dependent Variables}
{The dependent variables used in this study have been mentioned frequently up until this point, without reference to what it means to optimise those measures. A brief explanation is included below on a variable basis about what it means to increase or decrease these measures in real world terms. \\\\
%\subsubsection{Tobins Q score}
The Tobin's Q score is calculated as the market value of a company divided by the replacement value of a firm's assets. In theory, a value between 0 and 1 infers that the stock is undervalued since the cost to replace that companies assets is greater than the value of its stock. Such companies would then be attractive to buyers, who might purchase the company instead of establishing their own similar business. A Tobin's Q score over 1 infers that the company is overvalued, because the stock is more expensive than the replacement cost of its assets. These companies then might experience increased levels of competition from new businesses, who seek to capture some of the market share on offer and in turn reduce the Q score of the original company. It is clear then that a high Q score is neither inherently a good or bad thing, but rather a simple indicator whose significance relies on the interests of the interpreter. From an objective point of view, an optimal value is 1 and any extremes away from this neutral point can be considered suboptimal. Having said this, a large quantity of the literature refers to increases in, or large values of, the Q score as advantageous or desirable. This includes the original paper that this study is based on, and so a similar style is adopted here.   \\\\
%\subsubsection{Altman Z score}
The Altman Z score is more straightforward. The Z score, originally designed for firms in the manufacturing industry, gauges a companies likelihood of entering bankruptcy. This score can easily be discretised, since officially a score below 1.8 indicates a high risk of bankruptcy while a score above 3 indicated a low risk. Scores in-between cover a grey area of risk. Thus, optimising the Z score means increasing it towards 3. \\\\
%\subsubsection{Benish M score}
Finally, the Benish M score measures the likelihood that the reported earnings of a company have been intentionally manipulated. Overall, lowering the M score is optimal since a score below -2.22 (a greater negative) indicates the firm is unlikely to be manipulating earnings.  
   }
\subsection{Classification}
\cite{moldovan2015learning} used a number of metrics for measuring performance. They first show the accuracy of each model, which is simply the number of correctly identified observations in the data. Next they show the precision for each class, which is the number of correctly identified instances over the total predicted instances for that class. In the literature, these measures are also referred to the sensitivity and specificity. \\\\ Finally, they list the area under the receiver operator characteristic (ROC) curve. A ROC curve is created by plotting the sensitivity against the fall-out (or 1 - specificity) at various discrimination thresholds. The discrimination threshold describes the cutoff imposed on the predicted probabilities required for assigning an observation to a given class. The area under the curve (AUC) then, as described by \cite{flachAUC} is a single numerical measure of the model's performance, equivalent to the probability that a uniformly drawn random positive is ranked before a random negative. 
\subsection{Regression}
{As mentioned above, regularised linear regression is use in this study and varies with the penalty applied from ridge to lasso regression. The $r^2$ for each model is presented, as an indication of the quantity of the variance in that data that is described by the model. Alongside this, the root mean square error (RSME) is included as an measure of the difference between the values predicted by the model and the actual values observed.  Important to note here is that the RMSE is in the same units as the dependent variable it describes.  }
\subsection{Causal Estimation}\label{interpretationCausal}
{Section \ref{CausalEstimation-ResearchQuestions} outlines the research questions proposed for this section of the current study, and will form the basis for a number of result sets. For each question (and thus treatment and outcome pair), a number of performance related metrics are presented. \\\\
Each result set is presented following a uniform template. A header is given as an indication of the type of corporate governance features involved. The market for which the results relate is listed, followed by the dependent variable (or the outcome). The treatment is given in the form:
\begin{equation}
condition \quad ? \quad value \ if \ true \ : \ value \ if \ false
\end{equation}\\
The {\it condition} here is the specific combination of independent variables that dictate whether an observation is in the treated or control group.\\\\ 
After this the motivating statement from section \ref{CausalEstimation-ResearchQuestions} is given if applicable. In brackets, a flag is given for whether the results agree with the statement or not as well as whether this statement was originally made about this particular market or not ({\it original} or {\it not original}). This is purely to aid quick interpretation. \\\\
The causal estimate as per the matching process is then given, as a 95\% confidence interval. These figures represent the average treatment effect (ATE) of the intervention on the outcome. The ATE measures the difference in mean outcomes between the treated and control group, or in other words the average gain from treatment for units who were actually treated. A positive ATE indicates the treatment acted to increase the magnitude of the outcome measure, a negative the opposite. This portion of the study uses a binary outcome variable, and so the ATE can be interpreted as the percentage difference in the probability of either of the outcomes for a given unit. For example, an ATE of $15\%$ translates to an increase of $15\%$ in the probability of outcome 1. A $-15\%$ ATE translates to an increase of 15\% in the probability of obtaining outcome 0. For the outcome as measured by Tobin's Q score, a class of 1 translates to a higher than average value (or a inference that the stock is overvalued). For the outcome as measured by the Altman Z score, an outcome of 1 translates to a ``safe'' risk status. 
\\\\
Finally a series of plots are included showing the quality of that matching process on a covariate basis. Important here is an overlap on the x-axis between the two distributions. Significant overlap shows that the matching process was successful, finding equivalent observations in both the treated and control group to compare. }
